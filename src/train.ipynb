{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DefaultDataCollator\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "model_path = \"/home/hpc01/Marcos/Patch_Assesment/Model\"\n",
    "tokenizer_path = \"/home/hpc01/Marcos/Patch_Assesment/Tokenizer\"\n",
    "tokenized_dataset_path = \"/home/hpc01/Marcos/Patch_Assesment/Dataset/TokenizedDatasets/large\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenized_datasets = load_from_disk(tokenized_dataset_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, problem_type=\"single_label_classification\")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def create_batches(dataset, batch_size, pad_token_id):\n",
    "    \"\"\"\n",
    "    Iterador que divide el dataset en batches y añade padding para igualar las secuencias al tamaño máximo del batch.\n",
    "\n",
    "    Args:\n",
    "    - dataset: dataset tokenizado (e.g., tokenized_datasets[\"train\"]).\n",
    "    - batch_size: tamaño del batch.\n",
    "    - pad_token_id: id del token de padding (en este caso, el id de <|pad|>).\n",
    "\n",
    "    Yields:\n",
    "    - batch: un batch que contiene 'input_ids', 'attention_mask' y 'labels'.\n",
    "    \"\"\"\n",
    "    # Iterar sobre el dataset en pasos del tamaño del batch\n",
    "    for i in range(0, len(dataset['input_ids']), batch_size):\n",
    "        # Extraer el batch actual\n",
    "        batch_input_ids = dataset['input_ids'][i:i + batch_size]\n",
    "        batch_attention_mask = dataset['attention_mask'][i:i + batch_size]\n",
    "        batch_labels = dataset['labels'][i:i + batch_size]\n",
    "\n",
    "        # Encontrar la longitud máxima de 'input_ids' en el batch actual\n",
    "        max_length = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "\n",
    "        # Crear listas para almacenar los input_ids y attention_mask con padding\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "\n",
    "        # Aplicar padding a cada secuencia del batch\n",
    "        for input_ids, attention_mask in zip(batch_input_ids, batch_attention_mask):\n",
    "            # Calcular cuántos tokens de padding se necesitan\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            \n",
    "            # Rellenar con el token de padding\n",
    "            padded_input_ids.append(input_ids + [pad_token_id] * padding_length)\n",
    "            padded_attention_mask.append(attention_mask + [0] * padding_length)  # 0 para los tokens de padding\n",
    "        \n",
    "        # Yield del batch actual\n",
    "        yield {\n",
    "            'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(padded_attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(batch_labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")  # Obtener el id de <|pad|>\n",
    "batch_size = 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,  19741,    482,   5037,     11,     22,    489,   5037,     11,\n",
      "             22,  88220,   7163,    748,    320,   8514,      8,    341,  13342,\n",
      "           5939,   2594,   2986,    446,  21831,    311,   3047,    489,   2576,\n",
      "           5180,   1449,  13342,   5939,   2594,   2986,  22274,   2956,   3288,\n",
      "           7338,     77,      1,    489,    828,   5180,   1449,     12,    298,\n",
      "           5939,     13,    414,  40426,    396,   2312,  22274,    803,     10,\n",
      "            298,   5939,   2594,  40426,    396,   2312,  22274,    803,   7163,\n",
      "            197,    534,    720,   7163,    197,    322,   2175,    279,   2077,\n",
      "            198,  19741,    482,   9674,     11,     22,    489,   9674,     11,\n",
      "             22,  88220,   7163,    862,   4635,    280,    220,    197,    534,\n",
      "            720,     12,   2514,   1118,  11664,   1373,  16533,  83922,   1988,\n",
      "            340,     10,   2514,  11664,   1373,  16533,  83922,   1988,    340,\n",
      "          13342,  38706,   9530,     11,  34181,  57737,    341,   7163,  51496,\n",
      "          99165,    284,    502,  11664,    545,   7163,  96698,    602,    284,\n",
      "           1988,    280, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  19741,    482,   7261,     20,     11,     22,    489,   7261,\n",
      "             20,     11,     22,  88220,    260,    422,    320,  30919,    624,\n",
      "            854,      8,    341,   1835,    471,    905,    280,    260,    457,\n",
      "             12,    286,    422,    320,   1275,    366,    220,     15,   1393,\n",
      "           1963,   2669,  32955,   1996,      8,    341,     10,    286,    422,\n",
      "            320,   1275,    366,    220,     15,   1393,   1963,    871,  32955,\n",
      "           1996,      8,    341,   1835,    471,    905,    280,    260,    457,\n",
      "            260,    528,   1318,    502,  21570,    284,    502,    528,     58,\n",
      "          30919,   1996,    489,    220,     16,    947, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  19741,    482,  12375,     18,     11,     22,    489,  12375,\n",
      "             18,     11,     22,  88220,    260,   1620,   1796,  17179,   1767,\n",
      "             29,   3932,    284,    296,   1502,   2087,    673,   7283,    545,\n",
      "            260,    369,    320,    396,    602,    284,    220,     15,     26,\n",
      "            602,    366,   3932,   2546,   2178,    602,   2516,    341,   1835,\n",
      "           1620,  48857,   1217,    284,   3932,    673,   1998,    317,     12,\n",
      "            310,    369,    320,    396,    503,    284,    296,  15335,   8960,\n",
      "          12427,   1671,  58745,   2213,  12990,   2546,    368,    482,    220,\n",
      "             16,     26,    503,   2669,    220,     15,     26,    602,  21891,\n",
      "            341,     10,    310,    369,    320,    396,    503,    284,    296,\n",
      "          15335,   8960,  12427,   1671,  58745,   2213,  12990,   2546,    368,\n",
      "            482,    220,     16,     26,    503,   2669,    220,     15,     26,\n",
      "            503,  21891,    341,    338,    528,  51656,    284,    296,  15335,\n",
      "           8960,  12427,   1671,  58745,   2213,  12990,   4840,   1688,   3406,\n",
      "            317,    338,    528,  14959,    284,   2724,   7144,    673,  34607,\n",
      "           4374,   1801,     11,  51656,    317,    338,   2713,  11590,   2520,\n",
      "           2213,  42470,  50110,  38148,    317],\n",
      "        [128000,  19741,    482,   7743,     11,     22,    489,   7743,     11,\n",
      "             21,  88220,   7163,    748,   1533,    576,  18271,   1534,      8,\n",
      "            341,  13342,   2090,  82937,    284,  67863,   1420,  15040,  11971,\n",
      "            317,  13342,   2090,   6441,  19193,   4266,    284,  67863,   1420,\n",
      "           6441,  19193,   4266,  11971,    317,     12,    298,   2090,  53983,\n",
      "           4266,    284,  67863,   1420,  53983,   4266,  11971,    317,  13342,\n",
      "           2090,  18271,   1534,    284,    837,    280,   7163,    197,    534,\n",
      "            220,    197,   3818,  19741,    482,  10410,     11,     22,    489,\n",
      "          10410,     11,     21,  88220,    220,    197,    534,    220,   2514,\n",
      "            742,   3003,     17,    368,    341,   7163,   2090,    516,  15872,\n",
      "            284,  67863,   1420,    516,    771,  11971,    317,     12,    197,\n",
      "           2090,    516,    276,    275,  12509,    284,  67863,   1420,    516,\n",
      "            276,  64918,  11971,    317,   7163,   2090,    516,    771,  22133,\n",
      "            284,  67863,   1420,    516,    771,  22133,  11971,    317,   7163,\n",
      "           2090,  57834,  22133,    284,  67863,   1420,  57834,  22133,  11971,\n",
      "            317,   7163,   2090,   9472,  22133,    644,  70613,    284,  67863,\n",
      "           1420,  16199,  70613,  11971,   8295]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]]), 'labels': tensor([1, 1, 1, 0])}\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el iterador\n",
    "batch_gen_train = create_batches(tokenized_datasets[\"train\"], batch_size, pad_token_id)\n",
    "#batch_gen_test = create_batches(tokenized_datasets[\"test\"], batch_size, pad_token_id)\n",
    "\n",
    "\n",
    "batch = next(batch_gen_train)\n",
    "print(batch)\n",
    "#for k, v in batch.items():\n",
    "    #print(k,v.shape)\n",
    "\n",
    "#outputs = model(**batch)\n",
    "#print(outputs.loss, outputs.logits.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marcos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
